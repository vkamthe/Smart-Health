{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLSH-L3-Project_JTumelty.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JTumelty/Smart-Health/blob/main/Project-3/MLSH_L3_Project_JTumelty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFaS1ByoZCr9"
      },
      "source": [
        "Instructor: Juber Rahman<br>\n",
        "Dataset: St Petersburg INCART dataset from Physionet<br>\n",
        "Signal: 12-lead ECG<br>\n",
        "Pre-processing: neurokit<br>\n",
        "Method: 1-D CNN or Multi-head CNN<br>\n",
        "last updated: Nov 12, 2021<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzerJ1WDSt-6"
      },
      "source": [
        "# Project 3\n",
        "##Instructions:\n",
        "###Prework:\n",
        "1.  Watch a 12 lead ECG video https://www.youtube.com/watch?v=kwLbSx9BNbU \n",
        "\n",
        "> **Notes:** \n",
        ">\n",
        ">12 leads of the ECG represent the electrical signal viewed from 12 angles\n",
        "  * Limb leads look at the heart in a vertical plane and obtained from three electrodes (LA,RA, LL). \n",
        "    * Lead 1: Voltage between right arm and left arm (look at heart from left).\n",
        "    * Lead 2: Voltage between right arm and left leg (looks at heart from inferior left)\n",
        "    * Lead 3: Voltage between the left arm and the left leg (looks at heart from inferior right)\n",
        "    * aVR: unipolar (looks at upper right)\n",
        "    * aVL: unipolar (looks at upper left)\n",
        "    * aVF: unipolar (looks at inferior wall)\n",
        "  * Chest leads: view the heart in a horizontal plane and are unipolar leads \n",
        "    * Depolarisation toward a lead: positive\n",
        "    * Depolarisation away from a lead: negative\n",
        "\n",
        "\n",
        "\n",
        "2.  Select a 12 lead ECG database (e.g. St. \n",
        "Petersburg INCART Database) from Physionet and read the dataset descriptions to identify the heart patient types (e.g MI, CAD, TIA, Other)\n",
        "\n",
        "> **Notes:**\n",
        "> We use the St Petersburg INCART 12-lead Arrhythmia Database (https://doi.org/10.13026/C2V88N), where records have the following heart diseases.\n",
        "* **Acute MI---2**\n",
        "* **Transient ischemic attack (angina pectoris)---5**\n",
        "* **Prior MI---4**\n",
        "* **Coronary artery disease with hypertension---7 (4 with ECGs consistent with left ventricular hypertrophy)**\n",
        "* Sinus node dysfunction---1\n",
        "* Supraventricular ectopy---18\n",
        "* Atrial fibrillation or SVTA---3 (2 with paroxysmal AF)\n",
        "* WPW---2\n",
        "* AV block---1\n",
        "* Bundle branch block---3\n",
        ">\n",
        "> We have marked in bold the four conditions that we are particularly interested in classifying. \n",
        "\n",
        "3. Have a look at the project 3 starter notebook\n",
        "load and process the dataset using python WFDB, neurokit, biosppy etc. packages\n",
        "read about the different ECG channels and pick the best channel signal e.g. V2, V5 etc. \n",
        "Divide each ECG record into 1 minute chunks so in total you have 32 x no of hours x 60 ~ 23000 samples \n",
        "\n",
        "> **Notes:**\n",
        ">\n",
        "> From https://doi.org/10.1016/j.cmpb.2015.12.008, indicates lead II is frequently used in detecting heart diseases. Alternatively, lead V allows for classification of ventricular related arrhythmias. In this project, we will therefore consider using lead 2 and lead 5 separately.\n",
        "> From the summary data: https://physionet.org/lightwave/?db=incartdb/1.0.0, we find that V2 corresponds to channel 7 and V5 corresponds to channel 10.\n",
        "\n",
        "4. How these diseases MI, CAD, TIA etc. differ from each other, what morphological changes are introduced in the ECG signal? \n",
        "\n",
        "> **Notes:**\n",
        "> * MI: https://www.ahajournals.org/doi/epub/10.1161/CIR.0b013e31826e1058\n",
        ">   * Acute or evolving\n",
        "changes in the ST-T waveforms and Q waves. \n",
        ">   * New, or presumed new, J point elevation â±–0.1 mV is\n",
        "required in all leads other than V 2 and V 3. Note that there is a difference between men and women.\n",
        "> * CAD: **TO DO**\n",
        "> * TIA: **TO DO**\n",
        ">\n",
        "> Time permitting, we will later try to determine whether these properties agree with feature identification of the neural networks.\n",
        "\n",
        "5. Develop an articial neural network for multi-category heart disease classification from single channel ECG\n",
        "6. Develop a 1-D convolutional neural network for multi-category heart disease classification from single channel ECG\n",
        "7. Compare the performances of the developed deep learning models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g8OCNWULSN6"
      },
      "source": [
        "## Import packages and download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sL0-MAcvrxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a18375b-0e5c-4d06-9654-d22ca2202c8a"
      },
      "source": [
        "!pip install wfdb\n",
        "!pip install peakutils\n",
        "!pip install neurokit2\n",
        "import neurokit2 as nk\n",
        "import wfdb\n",
        "from wfdb import processing\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import peakutils"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.7/dist-packages (from wfdb) (3.5.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (1.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (21.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (3.0.6)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (4.28.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (7.1.2)\n",
            "Requirement already satisfied: setuptools-scm>=4 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (6.3.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->wfdb) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->wfdb) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.8.1->wfdb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.8.1->wfdb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.8.1->wfdb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.8.1->wfdb) (2021.10.8)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from setuptools-scm>=4->matplotlib>=3.3.4->wfdb) (1.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from setuptools-scm>=4->matplotlib>=3.3.4->wfdb) (57.4.0)\n",
            "Requirement already satisfied: peakutils in /usr/local/lib/python3.7/dist-packages (1.3.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from peakutils) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from peakutils) (1.19.5)\n",
            "Requirement already satisfied: neurokit2 in /usr/local/lib/python3.7/dist-packages (0.1.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from neurokit2) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (3.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (21.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (7.1.2)\n",
            "Requirement already satisfied: setuptools-scm>=4 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (6.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (4.28.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib->neurokit2) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from setuptools-scm>=4->matplotlib->neurokit2) (57.4.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from setuptools-scm>=4->matplotlib->neurokit2) (1.2.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->neurokit2) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->neurokit2) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->neurokit2) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjOb8p9_HrE9"
      },
      "source": [
        "Download all the WFDB records and annotations from a small Physionet Database: INCARTDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqZt3KRsvX4w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25fc72f4-baee-4259-b83e-2d6942fddce9"
      },
      "source": [
        "# Make a temporary download directory in your current working directory\n",
        "cwd = os.getcwd()\n",
        "dl_dir = os.path.join(cwd, 'tmp_dl_dir')\n",
        "\n",
        "# Download all the WFDB content\n",
        "wfdb.dl_database('incartdb', dl_dir=dl_dir)\n",
        "\n",
        "# Display the downloaded content in the folder\n",
        "display(os.listdir(dl_dir))\n",
        "\n",
        "# Cleanup: delete the downloaded directory\n",
        "#shutil.rmtree(dl_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating record list for: I01\n",
            "Generating record list for: I02\n",
            "Generating record list for: I03\n",
            "Generating record list for: I04\n",
            "Generating record list for: I05\n",
            "Generating record list for: I06\n",
            "Generating record list for: I07\n",
            "Generating record list for: I08\n",
            "Generating record list for: I09\n",
            "Generating record list for: I10\n",
            "Generating record list for: I11\n",
            "Generating record list for: I12\n",
            "Generating record list for: I13\n",
            "Generating record list for: I14\n",
            "Generating record list for: I15\n",
            "Generating record list for: I16\n",
            "Generating record list for: I17\n",
            "Generating record list for: I18\n",
            "Generating record list for: I19\n",
            "Generating record list for: I20\n",
            "Generating record list for: I21\n",
            "Generating record list for: I22\n",
            "Generating record list for: I23\n",
            "Generating record list for: I24\n",
            "Generating record list for: I25\n",
            "Generating record list for: I26\n",
            "Generating record list for: I27\n",
            "Generating record list for: I28\n",
            "Generating record list for: I29\n",
            "Generating record list for: I30\n",
            "Generating record list for: I31\n",
            "Generating record list for: I32\n",
            "Generating record list for: I33\n",
            "Generating record list for: I34\n",
            "Generating record list for: I35\n",
            "Generating record list for: I36\n",
            "Generating record list for: I37\n",
            "Generating record list for: I38\n",
            "Generating record list for: I39\n",
            "Generating record list for: I40\n",
            "Generating record list for: I41\n",
            "Generating record list for: I42\n",
            "Generating record list for: I43\n",
            "Generating record list for: I44\n",
            "Generating record list for: I45\n",
            "Generating record list for: I46\n",
            "Generating record list for: I47\n",
            "Generating record list for: I48\n",
            "Generating record list for: I49\n",
            "Generating record list for: I50\n",
            "Generating record list for: I51\n",
            "Generating record list for: I52\n",
            "Generating record list for: I53\n",
            "Generating record list for: I54\n",
            "Generating record list for: I55\n",
            "Generating record list for: I56\n",
            "Generating record list for: I57\n",
            "Generating record list for: I58\n",
            "Generating record list for: I59\n",
            "Generating record list for: I60\n",
            "Generating record list for: I61\n",
            "Generating record list for: I62\n",
            "Generating record list for: I63\n",
            "Generating record list for: I64\n",
            "Generating record list for: I65\n",
            "Generating record list for: I66\n",
            "Generating record list for: I67\n",
            "Generating record list for: I68\n",
            "Generating record list for: I69\n",
            "Generating record list for: I70\n",
            "Generating record list for: I71\n",
            "Generating record list for: I72\n",
            "Generating record list for: I73\n",
            "Generating record list for: I74\n",
            "Generating record list for: I75\n",
            "Generating list of all files for: I01\n",
            "Generating list of all files for: I02\n",
            "Generating list of all files for: I03\n",
            "Generating list of all files for: I04\n",
            "Generating list of all files for: I05\n",
            "Generating list of all files for: I06\n",
            "Generating list of all files for: I07\n",
            "Generating list of all files for: I08\n",
            "Generating list of all files for: I09\n",
            "Generating list of all files for: I10\n",
            "Generating list of all files for: I11\n",
            "Generating list of all files for: I12\n",
            "Generating list of all files for: I13\n",
            "Generating list of all files for: I14\n",
            "Generating list of all files for: I15\n",
            "Generating list of all files for: I16\n",
            "Generating list of all files for: I17\n",
            "Generating list of all files for: I18\n",
            "Generating list of all files for: I19\n",
            "Generating list of all files for: I20\n",
            "Generating list of all files for: I21\n",
            "Generating list of all files for: I22\n",
            "Generating list of all files for: I23\n",
            "Generating list of all files for: I24\n",
            "Generating list of all files for: I25\n",
            "Generating list of all files for: I26\n",
            "Generating list of all files for: I27\n",
            "Generating list of all files for: I28\n",
            "Generating list of all files for: I29\n",
            "Generating list of all files for: I30\n",
            "Generating list of all files for: I31\n",
            "Generating list of all files for: I32\n",
            "Generating list of all files for: I33\n",
            "Generating list of all files for: I34\n",
            "Generating list of all files for: I35\n",
            "Generating list of all files for: I36\n",
            "Generating list of all files for: I37\n",
            "Generating list of all files for: I38\n",
            "Generating list of all files for: I39\n",
            "Generating list of all files for: I40\n",
            "Generating list of all files for: I41\n",
            "Generating list of all files for: I42\n",
            "Generating list of all files for: I43\n",
            "Generating list of all files for: I44\n",
            "Generating list of all files for: I45\n",
            "Generating list of all files for: I46\n",
            "Generating list of all files for: I47\n",
            "Generating list of all files for: I48\n",
            "Generating list of all files for: I49\n",
            "Generating list of all files for: I50\n",
            "Generating list of all files for: I51\n",
            "Generating list of all files for: I52\n",
            "Generating list of all files for: I53\n",
            "Generating list of all files for: I54\n",
            "Generating list of all files for: I55\n",
            "Generating list of all files for: I56\n",
            "Generating list of all files for: I57\n",
            "Generating list of all files for: I58\n",
            "Generating list of all files for: I59\n",
            "Generating list of all files for: I60\n",
            "Generating list of all files for: I61\n",
            "Generating list of all files for: I62\n",
            "Generating list of all files for: I63\n",
            "Generating list of all files for: I64\n",
            "Generating list of all files for: I65\n",
            "Generating list of all files for: I66\n",
            "Generating list of all files for: I67\n",
            "Generating list of all files for: I68\n",
            "Generating list of all files for: I69\n",
            "Generating list of all files for: I70\n",
            "Generating list of all files for: I71\n",
            "Generating list of all files for: I72\n",
            "Generating list of all files for: I73\n",
            "Generating list of all files for: I74\n",
            "Generating list of all files for: I75\n",
            "Created local base download directory: /content/tmp_dl_dir\n",
            "Downloading files...\n",
            "Finished downloading files\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['I44.atr',\n",
              " 'I42.hea',\n",
              " 'I62.atr',\n",
              " 'I29.dat',\n",
              " 'I17.atr',\n",
              " 'I75.hea',\n",
              " 'I38.atr',\n",
              " 'I34.hea',\n",
              " 'I59.atr',\n",
              " 'I57.atr',\n",
              " 'I02.atr',\n",
              " 'I14.atr',\n",
              " 'I10.dat',\n",
              " 'I30.atr',\n",
              " 'I29.hea',\n",
              " 'I46.atr',\n",
              " 'I02.dat',\n",
              " 'I61.atr',\n",
              " 'I22.atr',\n",
              " 'I21.dat',\n",
              " 'I50.dat',\n",
              " 'I22.dat',\n",
              " 'I40.hea',\n",
              " 'I01.hea',\n",
              " 'I03.atr',\n",
              " 'I36.dat',\n",
              " 'I47.atr',\n",
              " 'I24.hea',\n",
              " 'I71.atr',\n",
              " 'I31.hea',\n",
              " 'I32.dat',\n",
              " 'I24.dat',\n",
              " 'I35.dat',\n",
              " 'I55.dat',\n",
              " 'I06.atr',\n",
              " 'I01.dat',\n",
              " 'I60.atr',\n",
              " 'I74.hea',\n",
              " 'I51.atr',\n",
              " 'I33.dat',\n",
              " 'I73.atr',\n",
              " 'I11.atr',\n",
              " 'I53.atr',\n",
              " 'I59.dat',\n",
              " 'I07.atr',\n",
              " 'I26.atr',\n",
              " 'I09.dat',\n",
              " 'I69.atr',\n",
              " 'I73.hea',\n",
              " 'I67.atr',\n",
              " 'I37.dat',\n",
              " 'I08.dat',\n",
              " 'I16.dat',\n",
              " 'I67.hea',\n",
              " 'I69.dat',\n",
              " 'I56.atr',\n",
              " 'I55.atr',\n",
              " 'I11.dat',\n",
              " 'I64.atr',\n",
              " 'I11.hea',\n",
              " 'I71.hea',\n",
              " 'I15.atr',\n",
              " 'I66.atr',\n",
              " 'I61.hea',\n",
              " 'I46.hea',\n",
              " 'I65.hea',\n",
              " 'I13.atr',\n",
              " 'I49.hea',\n",
              " 'I70.dat',\n",
              " 'I63.atr',\n",
              " 'I15.hea',\n",
              " 'I38.hea',\n",
              " 'I68.dat',\n",
              " 'I48.dat',\n",
              " 'I40.dat',\n",
              " 'I74.dat',\n",
              " 'I72.atr',\n",
              " 'I31.atr',\n",
              " 'I65.dat',\n",
              " 'I30.dat',\n",
              " 'I53.hea',\n",
              " 'I60.hea',\n",
              " 'I47.dat',\n",
              " 'I63.hea',\n",
              " 'I56.dat',\n",
              " 'I39.atr',\n",
              " 'I20.hea',\n",
              " 'I16.atr',\n",
              " 'I43.hea',\n",
              " 'I57.dat',\n",
              " 'I35.hea',\n",
              " 'I17.dat',\n",
              " 'I41.hea',\n",
              " 'I45.dat',\n",
              " 'I71.dat',\n",
              " 'I33.atr',\n",
              " 'I24.atr',\n",
              " 'I45.atr',\n",
              " 'I49.atr',\n",
              " 'I27.hea',\n",
              " 'I57.hea',\n",
              " 'I18.hea',\n",
              " 'I04.atr',\n",
              " 'I49.dat',\n",
              " 'I51.dat',\n",
              " 'I43.atr',\n",
              " 'I21.atr',\n",
              " 'I23.dat',\n",
              " 'I53.dat',\n",
              " 'I31.dat',\n",
              " 'I75.atr',\n",
              " 'I66.dat',\n",
              " 'I44.hea',\n",
              " 'I36.hea',\n",
              " 'I07.hea',\n",
              " 'I34.atr',\n",
              " 'I37.hea',\n",
              " 'I28.atr',\n",
              " 'I43.dat',\n",
              " 'I46.dat',\n",
              " 'I09.atr',\n",
              " 'I65.atr',\n",
              " 'I10.hea',\n",
              " 'I72.dat',\n",
              " 'I54.atr',\n",
              " 'I28.dat',\n",
              " 'I59.hea',\n",
              " 'I52.atr',\n",
              " 'I16.hea',\n",
              " 'I63.dat',\n",
              " 'I44.dat',\n",
              " 'I73.dat',\n",
              " 'I03.dat',\n",
              " 'I48.hea',\n",
              " 'I42.dat',\n",
              " 'I39.dat',\n",
              " 'I50.atr',\n",
              " 'I20.dat',\n",
              " 'I58.dat',\n",
              " 'I55.hea',\n",
              " 'I42.atr',\n",
              " 'I25.dat',\n",
              " 'I02.hea',\n",
              " 'I48.atr',\n",
              " 'I62.dat',\n",
              " 'I10.atr',\n",
              " 'I23.atr',\n",
              " 'I08.hea',\n",
              " 'I68.atr',\n",
              " 'I25.atr',\n",
              " 'I58.hea',\n",
              " 'I68.hea',\n",
              " 'I22.hea',\n",
              " 'I74.atr',\n",
              " 'I26.dat',\n",
              " 'I60.dat',\n",
              " 'I38.dat',\n",
              " 'I12.hea',\n",
              " 'I75.dat',\n",
              " 'I14.dat',\n",
              " 'I47.hea',\n",
              " 'I20.atr',\n",
              " 'I62.hea',\n",
              " 'I12.dat',\n",
              " 'I13.hea',\n",
              " 'I32.hea',\n",
              " 'I32.atr',\n",
              " 'I27.dat',\n",
              " 'I58.atr',\n",
              " 'I26.hea',\n",
              " 'I13.dat',\n",
              " 'I52.hea',\n",
              " 'I41.atr',\n",
              " 'I56.hea',\n",
              " 'I69.hea',\n",
              " 'I04.hea',\n",
              " 'I61.dat',\n",
              " 'I54.hea',\n",
              " 'I41.dat',\n",
              " 'I15.dat',\n",
              " 'I64.dat',\n",
              " 'I35.atr',\n",
              " 'I29.atr',\n",
              " 'I37.atr',\n",
              " 'I30.hea',\n",
              " 'I66.hea',\n",
              " 'I19.atr',\n",
              " 'I05.hea',\n",
              " 'I05.dat',\n",
              " 'I70.hea',\n",
              " 'I50.hea',\n",
              " 'I52.dat',\n",
              " 'I19.dat',\n",
              " 'I28.hea',\n",
              " 'I25.hea',\n",
              " 'I34.dat',\n",
              " 'I72.hea',\n",
              " 'I19.hea',\n",
              " 'I09.hea',\n",
              " 'I18.dat',\n",
              " 'I27.atr',\n",
              " 'I70.atr',\n",
              " 'I03.hea',\n",
              " 'I06.dat',\n",
              " 'I06.hea',\n",
              " 'I36.atr',\n",
              " 'I33.hea',\n",
              " 'I39.hea',\n",
              " 'I17.hea',\n",
              " 'I67.dat',\n",
              " 'I54.dat',\n",
              " 'I45.hea',\n",
              " 'I07.dat',\n",
              " 'I23.hea',\n",
              " 'I40.atr',\n",
              " 'I12.atr',\n",
              " 'I08.atr',\n",
              " 'I51.hea',\n",
              " 'I64.hea',\n",
              " 'I21.hea',\n",
              " 'I18.atr',\n",
              " 'I01.atr',\n",
              " 'I04.dat',\n",
              " 'I05.atr',\n",
              " 'I14.hea']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hToBUeQshloP"
      },
      "source": [
        "## Preprocessing for Neural Networks\n",
        "Using channel V2 as a starting point, we will read in all of the records, create epochs of 60s and make a dataframe combining this information with the disease classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcGM1HcLhllh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35406dd9-ccc9-4b04-de79-dde1c8669cd4"
      },
      "source": [
        "# Find the list of records to consider\n",
        "rec_list = sorted(list(set([os.path.splitext(x)[0] for x in os.listdir(dl_dir)])))\n",
        "print(rec_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I01', 'I02', 'I03', 'I04', 'I05', 'I06', 'I07', 'I08', 'I09', 'I10', 'I11', 'I12', 'I13', 'I14', 'I15', 'I16', 'I17', 'I18', 'I19', 'I20', 'I21', 'I22', 'I23', 'I24', 'I25', 'I26', 'I27', 'I28', 'I29', 'I30', 'I31', 'I32', 'I33', 'I34', 'I35', 'I36', 'I37', 'I38', 'I39', 'I40', 'I41', 'I42', 'I43', 'I44', 'I45', 'I46', 'I47', 'I48', 'I49', 'I50', 'I51', 'I52', 'I53', 'I54', 'I55', 'I56', 'I57', 'I58', 'I59', 'I60', 'I61', 'I62', 'I63', 'I64', 'I65', 'I66', 'I67', 'I68', 'I69', 'I70', 'I71', 'I72', 'I73', 'I74', 'I75']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MehCKidIJ1jW"
      },
      "source": [
        "def patient_info(rec_name,comments):\n",
        "  '''Classify patient disease:\n",
        "  Coronary artery disease: 0\n",
        "  MI: 1\n",
        "  Transient ischemic attack: 2\n",
        "  Other: 3\n",
        "  \n",
        "  Extract gender: M/F\n",
        "  Extract age\n",
        "  '''\n",
        "\n",
        "  # Disease\n",
        "  if 'Coronary artery disease' in comments[0]:\n",
        "    disease = 0\n",
        "  elif 'MI' in comments[0]:\n",
        "    disease = 1\n",
        "  elif 'Transient ischemic attack' in comments[0]:\n",
        "    disease = 2\n",
        "  else:\n",
        "    disease = 3\n",
        "\n",
        "  # Gender\n",
        "  if '<sex>: M' in comments[0]:\n",
        "    gender = 'M'\n",
        "  else:\n",
        "    gender = 'F'\n",
        "\n",
        "  # Age\n",
        "  age = int(fields['comments'][0][7:9])\n",
        "\n",
        "  return disease,age,gender"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpKc8fMIhsHL",
        "outputId": "4430f375-e2bd-4831-e874-c0e07b4e5ce4"
      },
      "source": [
        "# Iterate over all patient records and use cleaned ECG data \n",
        "# We further only consider 10 minutes out of the 30 minute records to make the analysis more manageable.\n",
        "for rec_name in rec_list:\n",
        "  lst = []\n",
        "  # Read in ECG record for channel V2\n",
        "  signals_7, fields = wfdb.rdsamp('/content/tmp_dl_dir/'+rec_name, channels=[7],sampto = 257*10*60+1)\n",
        "  # Extract patient info\n",
        "  disease,age,gender = patient_info(rec_name,fields['comments'])\n",
        "  # Process ECG signal\n",
        "  signals, info = nk.ecg_process(signals_7.flatten() , sampling_rate=257)\n",
        "  # Create epochs with 60 second intervals\n",
        "  epochs = nk.epochs_create(signals, sampling_rate=257, epochs_end=60)\n",
        "  # Iterate over epochs and append clean ecg signal with disease classification to dataframe\n",
        "  for e_key in epochs.keys():\n",
        "    ecg_clean = epochs[e_key].T.iloc[1].values.tolist()\n",
        "    lst.append(ecg_clean+[disease])\n",
        "    # Add data to datafame\n",
        "  all_data = pd.DataFrame(lst)\n",
        "  all_data.to_csv(rec_name+'_split.csv')\n",
        "\n",
        "  print(rec_name,disease)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I71 1\n",
            "I72 0\n",
            "I73 0\n",
            "I74 1\n",
            "I75 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndZMJwuQfqwZ"
      },
      "source": [
        "Load in all csv files and split into test and training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B3gyCw2fpqK",
        "outputId": "82823a20-c355-4e52-a185-868630c85b0f"
      },
      "source": [
        "rec_list = sorted([x for x in os.listdir('.') if x[-3:]=='csv' and x[0] == 'I'])\n",
        "all_records = pd.read_csv(rec_list[0])\n",
        "for rec_name in rec_list[1:]:\n",
        "  rec_join = pd.read_csv(rec_name)\n",
        "  all_records = pd.concat([all_records,rec_join],ignore_index=True)\n",
        "  print(rec_name, all_records.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I02_split.csv (20, 15422)\n",
            "I03_split.csv (30, 15422)\n",
            "I04_split.csv (40, 15422)\n",
            "I05_split.csv (50, 15422)\n",
            "I06_split.csv (60, 15422)\n",
            "I07_split.csv (70, 15422)\n",
            "I08_split.csv (80, 15422)\n",
            "I09_split.csv (90, 15422)\n",
            "I10_split.csv (100, 15422)\n",
            "I11_split.csv (110, 15422)\n",
            "I12_split.csv (120, 15422)\n",
            "I13_split.csv (130, 15422)\n",
            "I14_split.csv (140, 15422)\n",
            "I15_split.csv (150, 15422)\n",
            "I16_split.csv (160, 15422)\n",
            "I17_split.csv (170, 15422)\n",
            "I18_split.csv (180, 15422)\n",
            "I19_split.csv (190, 15422)\n",
            "I20_split.csv (200, 15422)\n",
            "I21_split.csv (210, 15422)\n",
            "I22_split.csv (220, 15422)\n",
            "I23_split.csv (230, 15422)\n",
            "I24_split.csv (240, 15422)\n",
            "I25_split.csv (250, 15422)\n",
            "I26_split.csv (260, 15422)\n",
            "I27_split.csv (270, 15422)\n",
            "I28_split.csv (280, 15422)\n",
            "I29_split.csv (290, 15422)\n",
            "I30_split.csv (300, 15422)\n",
            "I31_split.csv (310, 15422)\n",
            "I32_split.csv (320, 15422)\n",
            "I33_split.csv (330, 15422)\n",
            "I34_split.csv (340, 15422)\n",
            "I35_split.csv (350, 15422)\n",
            "I36_split.csv (360, 15422)\n",
            "I37_split.csv (370, 15422)\n",
            "I38_split.csv (380, 15422)\n",
            "I39_split.csv (390, 15422)\n",
            "I40_split.csv (400, 15422)\n",
            "I41_split.csv (410, 15422)\n",
            "I42_split.csv (420, 15422)\n",
            "I43_split.csv (430, 15422)\n",
            "I44_split.csv (440, 15422)\n",
            "I45_split.csv (450, 15422)\n",
            "I46_split.csv (460, 15422)\n",
            "I47_split.csv (470, 15422)\n",
            "I48_split.csv (480, 15422)\n",
            "I49_split.csv (490, 15422)\n",
            "I50_split.csv (500, 15422)\n",
            "I51_split.csv (510, 15422)\n",
            "I52_split.csv (520, 15422)\n",
            "I53_split.csv (530, 15422)\n",
            "I54_split.csv (540, 15422)\n",
            "I55_split.csv (550, 15422)\n",
            "I56_split.csv (560, 15422)\n",
            "I57_split.csv (570, 15422)\n",
            "I58_split.csv (580, 15422)\n",
            "I59_split.csv (590, 15422)\n",
            "I60_split.csv (600, 15422)\n",
            "I61_split.csv (610, 15422)\n",
            "I62_split.csv (620, 15422)\n",
            "I63_split.csv (630, 15422)\n",
            "I64_split.csv (640, 15422)\n",
            "I65_split.csv (650, 15422)\n",
            "I66_split.csv (660, 15422)\n",
            "I67_split.csv (670, 15422)\n",
            "I68_split.csv (680, 15422)\n",
            "I69_split.csv (690, 15422)\n",
            "I70_split.csv (700, 15422)\n",
            "I71_split.csv (700, 15422)\n",
            "I72_split.csv (710, 15422)\n",
            "I73_split.csv (720, 15422)\n",
            "I74_split.csv (730, 15422)\n",
            "I75_split.csv (740, 15422)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt7rro8c1Hj9"
      },
      "source": [
        "col_drop = all_records.columns[[0,-1]].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N1479mIjWPM"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osN-Ir1z0aW5"
      },
      "source": [
        "X = all_records.drop(col_drop, axis=1)\n",
        "y = all_records[col_drop[1]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.25, random_state=10 )"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGd6Oj1mzwKo"
      },
      "source": [
        "Save this dataframe to a csv file so they do not have to be regenerated each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcGHwsL3zulP"
      },
      "source": [
        "X_train.to_csv('X_train.csv')\n",
        "X_test.to_csv('X_test.csv')\n",
        "y_train.to_csv('y_train.csv')\n",
        "y_test.to_csv('y_test.csv')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XPSpVUTobDz"
      },
      "source": [
        "Check distribution of the classifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqyag8J8oaLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2e4f10-f2d9-4703-ec30-0667bcfa763d"
      },
      "source": [
        "y_train_count = {}\n",
        "y_test_count = {}\n",
        "for i in range(4):\n",
        "  y_train_count[i] = y_train[y_train.values == i].count()\n",
        "  y_test_count[i] = y_test[y_test.values == i].count()\n",
        "print('Training data:',y_train_count)\n",
        "print('Testing data:',y_test_count)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data: {0: 128, 1: 112, 2: 68, 3: 247}\n",
            "Testing data: {0: 42, 1: 38, 2: 22, 3: 83}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdXDhafXzUIT"
      },
      "source": [
        "Hence we see that the test and training have similar stratifications (as would be expected), but that the categories are unbalanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYmBZVqLhIhz"
      },
      "source": [
        "# Read in train and test data\n",
        "X_train= pd.read_csv('X_train.csv')\n",
        "X_test= pd.read_csv('X_test.csv')\n",
        "y_train = pd.read_csv('y_train.csv')\n",
        "y_test= pd.read_csv('y_test.csv')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNL7sJIqyF3_"
      },
      "source": [
        "# Drop 'Unnamed: 0' column\n",
        "unnamed='Unnamed: 0'\n",
        "X_train = X_train.drop(unnamed, axis=1)\n",
        "X_test = X_test.drop(unnamed, axis=1)\n",
        "y_train = y_train.drop(unnamed, axis=1)\n",
        "y_test = y_test.drop(unnamed, axis=1)\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TyeUOyo4QGX"
      },
      "source": [
        "## Artificial Neural Network\n",
        "Following code adapted from lab 3 notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzgjq0x7whTK"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import KFold, cross_val_score, KFold, StratifiedKFold"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4Vgs0Xq4Oqe"
      },
      "source": [
        "# class weighted neural network on an imbalanced classification dataset\n",
        "\n",
        "\n",
        "# define the neural network model\n",
        "def define_model(input_dim=None):\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\t# define first hidden layer and visible layer\n",
        "\tmodel.add(Dense(10, input_dim=input_dim, activation='relu', kernel_initializer='he_uniform'))\n",
        "\t# define output layer: note that because we are interested in a multi-class classification,\n",
        "\t# we dhould change the number of neurons in the final layer to 4: number of categories.\n",
        "\t# define loss and optimizer, which we have changed from binary_crossentropy to categorical_crossentropy\n",
        "\tmodel.add(Dense(4, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "def disp_cv_score(X_train,Y_train,epoch_no=100):\n",
        "\t'''Cross validation with 5-fold stratification, evaluating performance using f1 score'''\n",
        "\tn_input = X_train.shape[1]\n",
        "\tcv = StratifiedKFold(n_splits=5, shuffle=False) \n",
        "\tcv_score_total = 0\n",
        "\tkf = 0\n",
        "\tfor train_index, test_index in cv.split(X_train,Y_train):\n",
        "\t\tmodel = define_model(n_input)\n",
        "\t\tX1_train, X1_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "\t\tY1_train, Y1_test = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
        "\n",
        "\t\t# Convert labels data using encoder to fit the model\n",
        "\t\ty1_train_vec = np_utils.to_categorical(Y1_train)\n",
        "\t\tmodel.fit(X1_train, y1_train_vec,epochs=epoch_no,verbose=0)\n",
        "\t\n",
        "\t\t# Predict labels on test data\n",
        "\t\ty_pred = model.predict(X1_test)\n",
        "\t\t# Convert to single vector\n",
        "\t\tyclass = argmax(y_pred,axis = 1)\n",
        "\t\n",
        "\t\tscore = f1_score(Y1_test, yclass, average='macro')\n",
        "\t\tcv_score_total += score\n",
        "\t\tkf += 1\n",
        "\t\tprint('KFold ',kf,' F1 score: ', score,'\\n')\n",
        "\t\tprint(confusion_matrix(Y1_test['15420'].values, yclass))\n",
        "\n",
        "\tprint('Average F1 score: ',cv_score_total/5)"
      ],
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMsL-jzSMpsW",
        "outputId": "ddd0be34-a28a-4427-e708-a5ab7828afce"
      },
      "source": [
        "# Performance of model on KFold stratification\n",
        "disp_cv_score(X_train,y_train)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFold  1  F1 score:  0.43172541743970316 \n",
            "\n",
            "[[12  3  2  8]\n",
            " [ 6  6  4  6]\n",
            " [ 2  1  6  5]\n",
            " [11  3  7 29]]\n",
            "KFold  2  F1 score:  0.3111007330559319 \n",
            "\n",
            "[[ 8  5  5  7]\n",
            " [ 6  4  3  9]\n",
            " [ 0  2  7  5]\n",
            " [16 13  5 16]]\n",
            "KFold  3  F1 score:  0.27234589396012104 \n",
            "\n",
            "[[ 5 11  2  8]\n",
            " [ 5  7  2  8]\n",
            " [ 3  2  4  5]\n",
            " [12 13  8 16]]\n",
            "KFold  4  F1 score:  0.40334012392835916 \n",
            "\n",
            "[[ 8  7  4  7]\n",
            " [ 2  6  2 13]\n",
            " [ 3  3  6  1]\n",
            " [ 5 12  3 29]]\n",
            "KFold  5  F1 score:  0.323865961166493 \n",
            "\n",
            "[[ 7  6  4  9]\n",
            " [ 8  4  1 10]\n",
            " [ 4  3  5  1]\n",
            " [ 7  8  9 25]]\n",
            "Average F1 score:  0.3484756259101217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXVNZfUV7JRc"
      },
      "source": [
        "We see that the baseline model with one layer with 10 nodes has poor performance with wide variation. Fit the model on the full training data and display confusion matrix when predicted on the test data to understand where the performance issues come from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaFGhwF58CY6",
        "outputId": "13df26aa-f985-43e4-eeb1-cd82c2323b2f"
      },
      "source": [
        "model = define_model(n_input)\n",
        "history = model.fit(X_train, y_train_vec, epochs=100, verbose=0)\n",
        "yhat = model.predict(X_test)\n",
        "from numpy import argmax\n",
        "yclass = argmax(yhat,axis=1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "score = f1_score(y_test, yclass, average='macro')\n",
        "print('F1 score:',score)\n",
        "print(confusion_matrix(y_test['15420'].values, yclass))"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.38022568287808134\n",
            "[[12  9  4 17]\n",
            " [ 7 11  8 12]\n",
            " [ 5  4  7  6]\n",
            " [13 10  7 53]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yco2uhKvFyiB"
      },
      "source": [
        "Hence, we see that signals are classified fairly uniformly. Two further methods could improve the accuracy of the model:\n",
        "*   Rebalance the training data\n",
        "*   Vary the number of layers and the number of nodes in each layer\n",
        "In the following, we will consider the former."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uokkN3O-GfAd"
      },
      "source": [
        "# Rebalancing the training data by upsampling the minority classes\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "all_data = X_train\n",
        "all_data['label']=y_train\n",
        "\n",
        "df_0=all_data.iloc[y_train.index[y_train['15420'] == 0].tolist()]\n",
        "df_1=all_data.iloc[y_train.index[y_train['15420'] == 1].tolist()]\n",
        "df_2=all_data.iloc[y_train.index[y_train['15420'] == 2].tolist()]\n",
        "df_3=all_data.iloc[y_train.index[y_train['15420'] == 3].tolist()]\n",
        "df_1_upsample=resample(df_1,replace=True,n_samples=250,random_state=123)\n",
        "df_2_upsample=resample(df_2,replace=True,n_samples=250,random_state=124)\n",
        "df_0_upsample=resample(df_0,replace=True,n_samples=250,random_state=125)\n",
        "df_3_upsample=resample(df_3,replace=True,n_samples=250,random_state=123)\n",
        "\n",
        "train_df_resampled=pd.concat([df_0_upsample,df_1_upsample,df_2_upsample,df_3])\n",
        "\n",
        "#Shuffle data\n",
        "\n",
        "train_df_resampled = shuffle(train_df_resampled, random_state=12)\n",
        "\n",
        "# Split into signal and label\n",
        "y_rebalance = pd.DataFrame(train_df_resampled['label'])\n",
        "y_rebalance = y_rebalance.rename(columns = {'label':'15420'})\n",
        "train_df_resampled = train_df_resampled.drop(['label'],axis=1)\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzM9kW0NJSv4",
        "outputId": "ffd35d0e-55f0-4a89-aa8c-50912715be71"
      },
      "source": [
        "# Apply the model on the resampled training data\n",
        "disp_cv_score(train_df_resampled,y_rebalance,epoch_no=100)"
      ],
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFold  1  F1 score:  0.7162369459349022 \n",
            "\n",
            "[[33  8  0  9]\n",
            " [ 2 45  0  3]\n",
            " [ 0  0 50  0]\n",
            " [12 11  8 19]]\n",
            "KFold  2  F1 score:  0.8121323391560655 \n",
            "\n",
            "[[45  1  2  2]\n",
            " [ 3 42  0  5]\n",
            " [ 0  0 49  1]\n",
            " [ 5 11  6 28]]\n",
            "KFold  3  F1 score:  0.7809157163680013 \n",
            "\n",
            "[[46  0  2  2]\n",
            " [ 3 42  2  3]\n",
            " [ 0  0 50  0]\n",
            " [10 15  3 21]]\n",
            "KFold  4  F1 score:  0.7666398010803396 \n",
            "\n",
            "[[42  2  2  4]\n",
            " [ 5 44  0  1]\n",
            " [ 1  0 49  0]\n",
            " [ 9 11  8 21]]\n",
            "KFold  5  F1 score:  0.7411184968137119 \n",
            "\n",
            "[[37  3  4  6]\n",
            " [ 4 40  4  2]\n",
            " [ 0  0 50  0]\n",
            " [12  5  9 23]]\n",
            "Average F1 score:  0.7634086598706041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J41DeGs4UliR"
      },
      "source": [
        "Hence we see that performance has noticeably improved by balancing the classes. However, we notice that the final row indicates that normal heart beats are misclassified as belonging to those with a disease. This is potentially because each sample has only 60s worth of ecg signal and the abnormality may not be observed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6fa8debQ0tx",
        "outputId": "e275fee5-f02a-4f7b-ba75-bc492b76d50c"
      },
      "source": [
        "model = define_model(n_input)\n",
        "y_train_vec = np_utils.to_categorical(y_rebalance)\n",
        "history = model.fit(train_df_resampled,y_train_vec, epochs=65, verbose=0)\n",
        "yhat = model.predict(X_test)\n",
        "from numpy import argmax\n",
        "yclass = argmax(yhat,axis=1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "score = f1_score(y_test, yclass, average='macro')\n",
        "print('F1 score:',score)\n",
        "print(confusion_matrix(y_test['15420'].values, yclass))"
      ],
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.39390954103491627\n",
            "[[10  5  5 22]\n",
            " [ 6 12  6 14]\n",
            " [ 7  3  7  5]\n",
            " [ 6 13  5 59]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7MvnuvCdG_V"
      },
      "source": [
        "**This accuracy is low given the performance on each stratified Fold, but I can't work out where the error comes from. Likely that it arises from high n_samples particularly if repeated samples lie between a fold and testing data. This indicates that better performance could be obtained using the full dataset instead of a sample and oversampling.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOkTO31admCN"
      },
      "source": [
        "# Convolutional Neural Network\n",
        "We will now attempt this multi-class classification using the convolutional neural network model provided in the lab notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zinIL81_m3hW"
      },
      "source": [
        "from keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
        ")\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32q_2jvJle23"
      },
      "source": [
        "def network(X_train,y_train,X_test,y_test):\n",
        "    \n",
        "    im_shape=(X_train.shape[1],X_train.shape[2])\n",
        "\n",
        "    inputs_cnn=Input(shape=(im_shape), name='inputs_cnn')\n",
        "    conv1_1=Convolution1D(64, (6), activation='relu',input_shape=(im_shape))(inputs_cnn)\n",
        "    # Hidden layer as for ANN\n",
        "    #conv1_1=BatchNormalization()(conv1_1)\n",
        "    pool1=MaxPool1D(pool_size=(3), strides=(2), padding=\"same\")(conv1_1)\n",
        "\n",
        "    flatten=Flatten()(pool1)\n",
        "    # This reduces the dimensionality of the data, replacing groups by maximal value\n",
        "\n",
        " #   conv2_1=Convolution1D(64, (3), activation='relu', input_shape=(im_shape))(pool1)\n",
        " #   conv2_1=BatchNormalization()(conv2_1)\n",
        " #   pool2=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv2_1)\n",
        "\n",
        "#    conv3_1=Convolution1D(64, (3), activation='relu', input_shape=(im_shape))(pool2)\n",
        "#    conv3_1=BatchNormalization()(conv3_1)\n",
        "#    pool3=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv3_1)\n",
        "\n",
        "#    flatten=Flatten()(pool3)\n",
        "    \n",
        "    dense_end1 = Dense(64, activation='relu')(flatten)\n",
        "    dense_end1 = Dropout(0.5)(dense_end1)\n",
        "    dense_end2 = Dense(32, activation='relu')(dense_end1)\n",
        "    dense_end2 = Dropout(0.5)(dense_end2)\n",
        "    \n",
        "    main_output = Dense(4, activation='softmax', name='main_output')(dense_end2)\n",
        "    # Softmax returns the output probabilities for each class\n",
        "    \n",
        "    model = Model(inputs= inputs_cnn, outputs=main_output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    callbacks = [EarlyStopping(monitor='val_loss', mode='min',patience=8)]\n",
        "\n",
        "    y_train_vec = np_utils.to_categorical(y_train)\n",
        "    history=model.fit(X_train, y_train_vec,epochs=10,batch_size=32,callbacks=callbacks)\n",
        "\n",
        "    return(model,history)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaPPfJhnDyAa"
      },
      "source": [
        "Initial exploration indicated that without balancing the dataset, we find that the model tried to classify each result as the majority class, so we will attempt to use the rebalanced training data to fir the model, to see if this provides an improvement in perfomance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WMDa4PNs7V_"
      },
      "source": [
        "# Resize the testing and training data for use in CNN\n",
        "train_df_resampled = np.expand_dims(train_df_resampled, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWHadrwblpNG",
        "outputId": "20552eeb-4044-470f-c386-1ff0b9e1cdd8"
      },
      "source": [
        "model,history=network(train_df_resampled,y_rebalance,X_test,y_test)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inputs_cnn (InputLayer)     [(None, 15420, 1)]        0         \n",
            "                                                                 \n",
            " conv1d_42 (Conv1D)          (None, 15415, 64)         448       \n",
            "                                                                 \n",
            " max_pooling1d_42 (MaxPoolin  (None, 7708, 64)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 493312)            0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 64)                31572032  \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " main_output (Dense)         (None, 4)                 132       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,574,692\n",
            "Trainable params: 31,574,692\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 1.8218 - accuracy: 0.2728WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "32/32 [==============================] - 22s 667ms/step - loss: 1.8218 - accuracy: 0.2728\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 1.3791 - accuracy: 0.3350WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "32/32 [==============================] - 21s 650ms/step - loss: 1.3791 - accuracy: 0.3350\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 1.2776 - accuracy: 0.3701WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "32/32 [==============================] - 21s 652ms/step - loss: 1.2776 - accuracy: 0.3701\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 1.2386 - accuracy: 0.4042WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "32/32 [==============================] - 21s 661ms/step - loss: 1.2386 - accuracy: 0.4042\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 1.1534 - accuracy: 0.4293WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "32/32 [==============================] - 21s 654ms/step - loss: 1.1534 - accuracy: 0.4293\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 1.0885 - accuracy: 0.4895WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "32/32 [==============================] - 21s 654ms/step - loss: 1.0885 - accuracy: 0.4895\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 1.0478 - accuracy: 0.4835WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "32/32 [==============================] - 21s 659ms/step - loss: 1.0478 - accuracy: 0.4835\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.9793 - accuracy: 0.5476WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "32/32 [==============================] - 21s 663ms/step - loss: 0.9793 - accuracy: 0.5476\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.8918 - accuracy: 0.5898WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "32/32 [==============================] - 21s 664ms/step - loss: 0.8918 - accuracy: 0.5898\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.8289 - accuracy: 0.6018WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "32/32 [==============================] - 21s 667ms/step - loss: 0.8289 - accuracy: 0.6018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHWFK-CIlp86",
        "outputId": "fb24dbc3-3ecd-4f9f-cceb-5a6c0d7d9e6f"
      },
      "source": [
        "y_pred=model.predict(X_test)\n",
        "from numpy import argmax\n",
        "yclass = argmax(y_pred,axis=1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "score = f1_score(y_test, yclass, average='macro')\n",
        "print('F1 score:',score)\n",
        "print(confusion_matrix(y_test['15420'].values, yclass))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.46766712160960305\n",
            "[[ 8 10  0 24]\n",
            " [ 3 20  0 15]\n",
            " [ 8  2  8  4]\n",
            " [12  9  2 60]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZoNXi3G92Bh"
      },
      "source": [
        "We therefore see that the CNN model for disease classification has better performance than the ANN. We note, however, that the accuracy was still improving within the 10 epochs considered, so this could likely be improved. \n",
        "\n",
        "In this notebook, we have only considered a small selection of models, and further investigation would be warranted in order to improve the model:\n",
        "\n",
        "*   Use all data records instead of the subset that was considered here for testing purposes.\n",
        "*   While classification of diseases could have been higher, we should note that many of the records correspond to the same patient. Therefore, a final step in this work should be to average over all ECG records to give a final diagnosis.\n",
        "*   Further exploration into the architecture of the neural networks and the parameters involved would be helpful.\n",
        "* Enhanced investigation into the ECG records themselves: Should we weight features according to their importance and known results about distinguishing features? Other insights may be gained using domain knowledge.\n",
        "*   Finally, the models should be analysed to establish which features were most important in classifying the record.\n",
        "\n"
      ]
    }
  ]
}